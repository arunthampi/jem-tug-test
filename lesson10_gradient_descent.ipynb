{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lesson 10: Gradient Descent"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we were to pull a [cluster trace](https://code.google.com/p/googleclusterdata/) from production servers running right now in some randomly selected datacenter used for Big Data analytics, those resources are likely getting spent on the following four kinds of work:\n",
      "\n",
      "  * [moving data between different servers](http://stackoverflow.com/questions/12717659/hadoop-reduce-shuffle-merge-in-memory) (what others think your Data Scientists do)\n",
      "  * [cleaning up data prior to use, manually](http://www.amazon.com/dp/B008HMN5BE) (what your Data Scientists really do)\n",
      "  * [some optimization akin to gradient descent](http://www.umiacs.umd.edu/~jimmylin/publications/Lin_BigData2013.pdf) (what your Data Scientists think they do)\n",
      "  * [highly-available request/response services](http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf) (what your Data Scientists should probably think more about doing)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After we\u2019ve cleaned up our data, formulated workflows in terms of monoids, etc., done our graph representation, then parallelized it all with a wealth of linear algebra, the heavy-lifting that remains for our myriad of computers to perform is probably something closely akin to gradient descent..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "When you have a spare hour, definitely check out this ACM talk where Jeff Dean describes a large use case for the many layers of neural nets used in [deep learning](https://plus.google.com/+ResearchatGoogle/posts/C1dPhQhcDRv) at Google. We\u2019re probably all going to be using some variant of [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) for a long, long while."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" src=\"//www.youtube.com/embed/nK6daeTZGA8\" width=\"560\"> </iframe>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s take a look at how that works \u2013 and here\u2019s where we actually get to apply some Calculus! Happy happy, joy joy. Suppose that in our tablet/drone cocktail delivery start-up, based on extensive analysis of large-scale data, we have found that the cost of losing [cocktail parasol drink umbrellas](http://www.amazon.com/dp/B000H2XXAA) turns out to be a function of the height at which the drones fly. Perhaps we used least squares approximation to handle that curve fitting. Or something."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Anywho, suppose that the following *f(x)* approximates our rate of drink umbrella loss as a function of the drones\u2019 mean flying height *x*:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align*} f(x)=x^{ 4 }-3x^{ 3 }+2 \\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That defines an objective function which we want to minimize. Frankly, we could use calculus at that point to determine the solution for a simple function like this. But let\u2019s pretend the problem is something harder, something much much harder."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gradient_descent (x1, toler, step_size, max_iter, func, func_deriv):\n",
      "    x0 = 0\n",
      "    print \"\\t\".join([ \"i\", \"x1\", \"error\", \"func(x1)\" ])\n",
      "\n",
      "    for i in xrange(max_iter):\n",
      "        error = abs(x1 - x0)\n",
      "        val = (i, x1, error, func(x1))\n",
      "        print \"\\t\".join([ \"%d %0.4e %0.4e %3.4e\" % val ])\n",
      "\n",
      "        if abs(x1 - x0) <= toler:\n",
      "            return x1\n",
      "\n",
      "        x0 = x1\n",
      "        x1 = x0 - step_size * func_deriv(x0)\n",
      "\n",
      "    return \"halted\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Bokay. That was not particularly difficult. One big loop, really. Now let's define the objective function, and for QA purposes we'll also define its first derivative:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def f (x):\n",
      "    return x**4 - 3 * x**3 + 2\n",
      "\n",
      "def f_deriv (x):\n",
      "    \"\"\"first derivative of f(x)\"\"\"\n",
      "    return 4 * x**3 - 9 * x**2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we define a few parameters, then run the code to optimize *f(x)* for an answer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x1 = 6\n",
      "toler = 0.00001\n",
      "step_size = 0.01\n",
      "max_iter = 93\n",
      "\n",
      "# inputs: f, starting value x1, termination tolerances\n",
      "min_height = gradient_descent(x1, toler, step_size, max_iter, f, f_deriv)\n",
      "\n",
      "print \"min height for drone flights %0.2f\" % (min_height)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clearly, we must attempt to fly our drones at a height of approximately 2.25 meters, to minimize the loss of drink umbrellas. Hopefully we won\u2019t run into any Olympic basketball players. Hopefully."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Gradient descent is a *deterministic* algorithm, in other words you get the same results every time you run GD with the same data, at the same cost. In practice, problems become much more complex than what\u2019s shown above. In lieu of having a well-defined, differentiable function *f(x)*, we\u2019re probably creating some classifier based on a training set of data, working with lots of approximations, error terms, etc. In other words, building a *learner*. While it may involve some mathematical tricks and transforms to arrive at a differentiable objective function, the basic idea is to tune parameters in the learner such that it:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1.  minimizes error when the learner\u2019s predictions are compared with the training set\n",
      "2.  penalizes solutions that veer toward *overfitting* (i.e., regularization)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With GD, we must run through the entire training set on each iteration. That\u2019s expensive. Instead we generally use a faster variant called [stochastic gradient descent](http://scikit-learn.org/stable/modules/sgd.html) (SGD). Being a *stochastic* algorithm, it hops and skips its way gracefully through the training set, \u201clearning\u201d from each new data point encountered. SGD is more efficient, but it introduces more knobs and dials to adjust. It also becomes sensitive to how the data is prepared, in terms of [feature scaling](http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling). There are trade-offs in life."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For a great deep-dive on SGD, check out this excellent [Python tutorial](http://www.iro.umontreal.ca/~pift6266/H10/notes/gettingstarted.html#stochastic-gradient-descent) by Yoshua Bengio @Universit\u00e9 de Montr\u00e9al. Also check out the [scikit-learn](http://scikit-learn.org/stable/modules/sgd.html) examples. Good stuff."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}