{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lesson 08: Algebraic Graphy Theory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s talk a bit about graphs\u2026 Graphs are composed of *vertices* (the nodes in the graph) and *edges* (the arcs between nodes). We can construct a special kind of matrix, called an *adjacency matrix*, which has a row for each vertex and a column for each vertex. The elements of an adjacency matrix have a `1` if an edge exists between the respective pair of vertices, and a `0` otherwise."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check out [An Introduction to Algebraic Graph Theory](http://buzzard.ups.edu/talks/beezer-2009-pacific-agt.pdf) by Rob Beezer, starting on slide 9. There are four vertices in the graph `[u, v, w, x]` and an adjacency matrix defined as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align*} \\textbf{A} = \\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 1 & 0 & 1 & 1 \\\\ 0 & 1 & 0 & 1 \\\\ 1 & 1 & 1 & 0 \\\\ \\end{bmatrix} \\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An adjacency matrix has certain properties. For example, we know that it is symmetric and that it has real eigenvalues. Using the `eig` method in NumPy we can find the eigenvalues and eigenvectors \u2013 you know the routine:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "A = np.array([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1]).reshape(4, 4)\n",
      "eig = np.linalg.eig(A)\n",
      "\n",
      "print A\n",
      "print eig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, we can haz eigenvalues. Keep in mind that the eigenvalues do not define the graphs \u2013 different graphs could have the same eigenvalues. Even so, we can put those eigenvalues to good use in measuring graphs. For example, a [graph diameter](http://mathworld.wolfram.com/GraphDiameter.html) is defined as the \u201clongest shortest path\u201d \u2013 in other words, the max number of vertices which must be traversed to go from one vertex to another when paths do not backtrack. A graph of diameter `d` has `d + 1` distinct eigenvalues."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Problem:** For the graph represented by matrix <strong>A</strong>, modify the following Python code to calculate its diameter:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n = len(set(eig[0]))\n",
      "\n",
      "# not quite correct, yet...\n",
      "print \"graph diameter\", n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many more was to leverage eigenvalues, this is just a quick example. A key takeaway that we\u2019ve previously examined a bridge between abstract algebra and linear algebra, and now here we have a bridge between linear algebra and graph theory."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Deconstructing Page Rank"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s consider the PageRank algorithm, taking a look at [The Mathematics of Google Search](http://www.math.cornell.edu/~mec/Winter2009/RalucaRemus/Lecture3/lecture3.html) by Raluca Tanase and Remus Radu at Cornell. Note their link graph, and also \\\\(\\\\textbf{A}\\\\) as a *transition matrix* for that graph:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align*} \\textbf{A} = \\begin{bmatrix} 0 & 0 & 1 & 1/2 \\\\ 1/3 & 0 & 0 & 0 \\\\ 1/3 & 1/2 & 0 & 1/2 \\\\ 1/3 & 1/2 & 0 & 0 \\\\ \\end{bmatrix} \\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is similar to an adjacency matrix, except that the edges have weights instead of the `1` and `0` values for the matrix elements. In that case, one way to think about how PageRank works is to consider an equation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align*} \\textbf{ A }^{ \\textbf{N} }\\textbf{ v }=\\textbf{ v } \\end{align*} "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In other words, find the eigenvector <strong>v</strong> where the eigenvalue &lambda; = 1, and that eigenvector will represent the ranks for each web page in the graph. Take its max element to determine the top-ranked page. To calculate ranks, we can iterate until the system reaches equilibrium \u2013 in other words, until the values in the eigenvector <strong>v</strong> stop changing appreciably."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = np.array([0., 0., 1., 1/2., 1/3., 0., 0., 0., 1/3., 1/2., 0., 1/2., 1/3., 1/2., 0., 0.]).reshape(4, 4)\n",
      "\n",
      "N = 200\n",
      "A = np.linalg.matrix_power(A, N)\n",
      "eig = np.linalg.eig(A)\n",
      "\n",
      "print A\n",
      "print eig"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Problem:** Given the transition matrix **A** shown above, what is the rank metric for the top-ranked web page?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is substantially more to PageRank (e.g, *damping factor*), plus there are much [more optimal ways](http://en.wikipedia.org/wiki/PageRank#Power_Method) to calculate it \u2013 which in turn make better use of eigenvalues. But the point is clear: linear algebra, FTW."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Singular Value Decomposition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider a matrix factorization called [Singular Value Decomposition](http://www.ling.ohio-state.edu/~kbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf), based on the matrix form:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\\begin{align*} \\textbf{M}=\\textbf{U}\\textbf{\u03a3}\\textbf{ V }^{ \\textbf{H} } \\end{align*} "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We could go into gory detail about how to construct the components of those matrices using eigenvalues and eigenvectors\u2026 for example, \u03a3 is a diagonal matrix \u2013 all zeros except for its diagonal, which is the square roots of the eigenvalues of MHM arranged in decreasing order, called *singular values*\u2026 Moreover, these components can be created efficiently in parallel using techniques such as *QR factorization*\u2026"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The gist of this is to reduce a high dimensional, highly variable set of data points down to a lower dimensional space, exposing the structure in the original data more clearly. So let's give that a go \u2013 here's an example based on a blog post by [Dr Skippy](http://blog.drskippy.com/2012/05/14/dimension-reduction-for-machine-learning-simple-example-of-svd-pca-pathology/), plus or minus a few corrections:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.linalg as sp\n",
      "\n",
      "X_t = mat([ [1, 3, -10, 23], [0, 2, 1, 5], [-1, -3, 9, -11], [0, -2, 0, 3 ] ])\n",
      "X = X_t.T\n",
      "print X\n",
      "\n",
      "W, s, V_h = np.linalg.svd(X)\n",
      "print \"singular values\", s"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note the singular values\u2026 we can prune the smallest singular values of \u03a3 to scale down the SVD \u2013 dimensional reduction, and probably substantial reduction of data, with the essential structure of the raw data preserved. In the following example, use the products of the SVD to reconstruct the matrix <strong>X</strong> and compare the difference:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sigma = sp.diagsvd(s, len(X), len(V_h))\n",
      "X_recon = np.dot(np.dot(W, sigma), V_h)\n",
      "\n",
      "print abs(X - X_recon)\n",
      "print \"max difference %0.3e\" % (np.max(abs(X - X_recon)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In contrast, the following truncates \u03a3 by setting its smallest singular value to zero, then uses the products of the SVD to reconstruct the matrix <strong>X</strong> and compares the difference:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1 = list(s)\n",
      "s1[3] = 0\n",
      "sigma = sp.diagsvd(s1, len(X), len(V_h))\n",
      "X_recon = np.dot(np.dot(W, sigma), V_h)\n",
      "\n",
      "print abs(X - X_recon)\n",
      "print \"max difference %0.3e\" % (np.max(abs(X - X_recon)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There is a difference, but that's comparable with round-off error, so far."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Installation notes:*\n",
      "\n",
      "  * [NumPy](https://store.continuum.io/cshop/anaconda/)\n",
      "  * [SciPy](http://stackoverflow.com/questions/2213551/installing-scipy-with-pip)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}