{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lesson 07: Dimensional Reduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How do we use eigenvalues and eigenvectors? Oh, in so many ways! Let\u2019s start with the notion of matrix *decomposition*, for example used in [principal component analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) (PCA)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recall the [Breiman paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.156.4933&rep=rep1&type=pdf) in a previous chapter, where many input \\\\(\\\\textbf{x}\\\\)\u2019s go into a \u201cblack box\u201d for algorithmic modeling. One issue that we grapple with continually in machine learning is *dimensional reduction*. In other words, modeling with 1000 different \\\\(\\\\textbf{x}\\\\)\u2019s may become a problem \u2013 performance costs come to mind \u2013 while, say, 20 \\\\(\\\\textbf{v}\\\\)\u2019s could work just fine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use eigenvectors for the PCA calculations. Visualize a high-dimensional space, with all of those 1000 different \\\\(\\\\textbf{x}\\\\)\u2019s \u2013 essentially looking at a \u201ccloud\u201d of data points that compares their *covariance*. We can fit a straight line through that cloud of points, as best as possible. Oh, here\u2019s an idea\u2026 how about we use a *least squares approximation* to solve that? Great. The resulting line defines an eigenvector, which we will call the first *principal component* in this context."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<p><center><a href=\"http://en.wikipedia.org/wiki/Principal_component_analysis\"><img style=\"width:33%\" src=\"http://upload.wikimedia.org/wikipedia/commons/1/15/GaussianScatterPCA.png\"/><br/>Wikipedia entry</a></center></p>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Elements of that eigenvector become the coefficients for a polynomial of the 1000 different \\\\(\\\\textbf{x}\\\\)\u2019s \u2013 so we get an equation. That equation becomes one of the \\\\(\\\\textbf{v}\\\\)\u2019s, as a kind of synthesized variable. Then we can peel off the eigenvector from the high-dimensional cloud, and apply the least squares trick again. Great, we get another polynomial as a synthesized variable \u2013 as the second principal component. Rinse, lather, repeat. After 20 iterations we will have 20 principal components. Each of those reduces the 1000 different \\\\(\\\\textbf{x}\\\\)\u2019s (high dimension) into 20 different \\\\(\\\\textbf{v}\\\\)\u2019s (low dimension). Ergo the name, dimensional reduction. There are many variants, and frankly [clustering algorithms](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) do pretty much the same trick, just in a different way."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Problem:** a QA report shows 4 judges scoring drinks by our **Top Bartendrs** (4 measurements, 15 observations) ...\n",
      "instead of presenting this QA data as a 4-D graph (tried that, not good) let\u2019s use _PCA_ to explain just the most important details. Let\u2019s simplify that report to two dimensions instead, using the first two principal components."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat <<EOF > data/judges.csv\n",
      "carol,5.1,3.5,1.4,0.2\n",
      "carol,4.9,3.0,1.4,0.2\n",
      "carol,4.7,3.2,1.3,0.2\n",
      "carol,4.6,3.1,1.5,0.2\n",
      "deepali,5.7,3.0,4.2,1.2\n",
      "deepali,5.7,2.9,4.2,1.3\n",
      "deepali,6.2,2.9,4.3,1.3\n",
      "deepali,5.1,2.5,3.0,1.1\n",
      "deepali,5.7,2.8,4.1,1.3\n",
      "kirill,6.5,3.0,5.5,1.8\n",
      "kirill,7.7,3.8,6.7,2.2\n",
      "kirill,7.7,2.6,6.9,2.3\n",
      "kirill,6.0,2.2,5.0,1.5\n",
      "kirill,6.9,3.2,5.7,2.3\n",
      "kirill,5.6,2.8,4.9,2.0\n",
      "EOF"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's define a Python function to compute PCA for a given matrix \\\\(\\\\textbf{A}\\\\), based on a great example at [JustGlowing](http://glowingpython.blogspot.it/2011/07/pca-and-image-compression-with-numpy.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def princomp (A):\n",
      "  \"\"\"compute the eigenvalues and eigenvectors of a covariance matrix\"\"\"\n",
      "  # subtract the mean (along columns)\n",
      "  M = (A - np.mean(A.T, axis=1)).T\n",
      "  (latent, coeff) = np.linalg.eig(np.cov(M))\n",
      "\n",
      "  # sort the eigenvalues into ascending order\n",
      "  p = size(coeff, axis=1)\n",
      "  idx = argsort(latent)\n",
      "  idx = idx[::-1]\n",
      "\n",
      "  # sort the eigenvectors to correspond to their eigenvalues\n",
      "  coeff = coeff[:, idx]\n",
      "  latent = latent[idx]\n",
      "\n",
      "  return M, coeff, latent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, some brief code to read the data into a matrix..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "score = []\n",
      "tendr = []\n",
      "\n",
      "with open(\"data/judges.csv\") as f:\n",
      "  print \"tendr\\tjudge1\\tjudge2\\tjudge3\\tjudge4\"\n",
      "\n",
      "  for line in f:\n",
      "    v = line.strip().split(\",\")\n",
      "    tendr.append(v.pop(0))\n",
      "    score.append(map(lambda x: float(x), v))\n",
      "\n",
      "A = np.array(score)\n",
      "print A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Then we analyze the judges' scores for our bartendrs, calling the PCA function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "M, eigenvectors, eigenvalues = princomp(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check out the resulting *eigenvalues*, which represent how much the corresponding *eigenvectors* (principal components) explain the variance in the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print eigenvalues"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's visualize that list, to show the diminishing returns of using PCA for dimensional reduction:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pylab as pl\n",
      "\n",
      "sum_eig = sum(eigenvalues)\n",
      "portion_variance = map(lambda x: (x / sum_eig), sorted(eigenvalues, reverse=True))\n",
      "\n",
      "pl.plot(map(lambda x: 1.0 - x, portion_variance), '-.')\n",
      "pl.xlabel('principal component')\n",
      "pl.ylabel('portion variance')\n",
      "pl.xticks(np.arange(1, len(eigenvalues) + 1, 1.0))\n",
      "pl.grid()\n",
      "\n",
      "pl.show()\n",
      "\n",
      "tally = 0.0\n",
      "i = 0\n",
      "\n",
      "for portion in portion_variance:\n",
      "    i += 1\n",
      "    tally += portion\n",
      "    print \"%2d principal components represent %6.2f%% of the variance in the data\" % (i, tally * 100.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In other words, the first two principal components explain 98.58% of the variance in the QA report. Good enough! Let's take a look at the corresponding *eigenvectors*:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print eigenvectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll use those to transform the raw data (judges' scores), projecting it into a lower dimensional space:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eigenvectors = eigenvectors[:, range(2)]\n",
      "score = dot(eigenvectors.T, M)\n",
      "\n",
      "print score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We use that transformed, lower dimensional data set so that the *Foobartendr.io* data science team can now make a most epic 2-D visualization to compare each **Top Bartendr** "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tendr_transform = { \"carol\": 'r', \"deepali\": 'b', \"kirill\": 'y' }\n",
      "colors = map(lambda x: tendr_transform[x], tendr)\n",
      "\n",
      "pl.scatter(score[0], score[1], c=colors, marker='x', label=tendr)\n",
      "pl.xlabel('princomp 2')\n",
      "pl.ylabel('princomp 1')\n",
      "pl.grid()\n",
      "\n",
      "p1 = Rectangle((0, 0), 1, 1, facecolor='r')\n",
      "p2 = Rectangle((0, 0), 1, 1, facecolor='b')\n",
      "p3 = Rectangle((0, 0), 1, 1, facecolor='y')\n",
      "\n",
      "pl.legend((p1, p2, p3,), (\"carol\", \"deepali\", \"kirill\",))\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clearly, this diagram shows that Carol has been taking names and kicking drones, as our reigning **Top Bartendr**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Installation notes:*\n",
      "\n",
      "  * [NumPy, Matplotlib](https://store.continuum.io/cshop/anaconda/)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}